## üï∑Ô∏è Web Crawler Avan√ßado

### üöÄ Recursos Principais

- **Renderiza√ß√£o Din√¢mica**: Suporte a p√°ginas com JavaScript
- **M√∫ltiplos Dom√≠nios**: Crawling flex√≠vel 
- **Controle Preciso**: Parametriza√ß√£o de profundidade e p√°ginas
- **Tolerante a Falhas**: Fallback entre Selenium e requests

### üõ† Uso R√°pido

```bash
# Crawler para um dom√≠nio espec√≠fico
docker-compose run crawler exemplo.com.br

# Configura√ß√µes personalizadas
docker-compose run crawler exemplo.com.br --max-pages 20 --max-depth 4
docker-compose run crawler exemplo.com.br --max-pages 10 --max-depth 2
```

### üîß Par√¢metros

| Par√¢metro | Descri√ß√£o | Padr√£o |
|-----------|-----------|--------|
| `--output-dir` | Diret√≥rio de resultados | `crawled_data` |
| `--js-render-time` | Tempo renderiza√ß√£o JS | 3s |
| `--delay` | Intervalo requisi√ß√µes | 1s |
| `--max-pages` | P√°ginas m√°ximas | 50 |
| `--max-depth` | Profundidade crawling | 2 |

### ‚ö†Ô∏è Avisos

- Respeite `robots.txt`
- Configure delays adequados
- Use eticamente